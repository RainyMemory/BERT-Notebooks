{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('envPytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "82c25c09a0a8ff8dd25a51ab110a5b27617daf1cc0c39e1b255c757f044d7c3e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# A pre-trained Bert model use case\n",
    "\n",
    "- The pre-trained model comes from Huggingface\n",
    "- The dataset comes from kaggle 'Sentiment140 dataset with 1.6 million tweets'\n",
    "\n",
    "## Exploring the dataset\n",
    "\n",
    "- Read in the file\n",
    "- I took 1000 tweets, 500 from positive samples and 500 from negative samples\n",
    "- Save them to a new file, I try to catch the characteristics of the data by watching the 1000 samples"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re \n",
    "\n",
    "ori_file_path = \"D:/CodeBase-User/VScode-workspace/BERT-Notebooks/Dataset/training.1600000.processed.noemoticon.csv\"\n",
    "slice_file_path = \"D:/CodeBase-User/VScode-workspace/BERT-Notebooks/Dataset/noemoticon-slice-1000.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ori = pd.read_csv(ori_file_path, encoding='ISO-8859-1', header=None)\n",
    "df_ori.columns = ['sentiment','id','date','flag','user','tweet']\n",
    "df_ori.sentiment = df_ori.sentiment.map({4:1,0:0})\n",
    "df = pd.concat([df_ori[df_ori.sentiment == 0][:500], df_ori[df_ori.sentiment == 1][:500]])\n",
    "df.to_csv(slice_file_path)"
   ]
  },
  {
   "source": [
    "- Here is an interesting problem of the dataset, \n",
    "- There exist some samples that labeled both posistive and negative."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same tweet but have different labels (both 0 and 1)\n",
    "df = pd.read_csv(slice_file_path)\n",
    "tweet_unique = df.tweet.drop_duplicates(keep=False)\n",
    "records_with_same_tweet = df[True ^ df.tweet.isin(tweet_unique)]\n",
    "records_with_same_tweet[['sentiment', 'user', 'tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[True ^ df.tweet.isin(records_with_same_tweet.tweet)]\n",
    "df.sentiment.value_counts().plot(kind='bar')"
   ]
  },
  {
   "source": [
    "- Check the pretty common 'special tokens' in tweets, such as topics `#`, mentions `@`, urls `http/https` and characters suffer from encoding errors `&`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_mention = r'^@[0-9a-zA-Z_]+'\n",
    "pattern_topic = r'^#[0-9a-zA-Z]+'\n",
    "pattern_sign = r'^&[a-z]+;'\n",
    "pattern_url = r'^http\\S+'\n",
    "mention_count = 0\n",
    "for tweet in df.tweet:\n",
    "    mention_res = re.match(pattern_mention, tweet, flags=re.IGNORECASE|re.S)\n",
    "    if mention_res:\n",
    "        mention_count += len(mention_res.group())\n",
    "    topic_res = re.match(pattern_topic, tweet, flags=re.IGNORECASE|re.S)\n",
    "    if topic_res:\n",
    "        print(topic_res.group())\n",
    "    sign_res = re.match(pattern_sign, tweet, flags=re.IGNORECASE|re.S)\n",
    "    if sign_res:\n",
    "        print(sign_res.group())\n",
    "    url_res = re.match(pattern_url, tweet, flags=re.IGNORECASE|re.S)\n",
    "    if url_res:\n",
    "        print(url_res.group())\n",
    "print(\"The number of mentions: \", mention_count)    "
   ]
  },
  {
   "source": [
    "- Before doing text cleaning, I check the frequency of the words in the tweets\n",
    "- Just to see if this would change after cleaning the data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = df.tweet.apply(lambda tweet: len([word for word in str(tweet).lower().split()]))\n",
    "df_stopwords = df.tweet.apply(lambda tweet: len([word for word in str(tweet).lower().split() if word in stop_words]))\n",
    "df_non_stopwords = df.tweet.apply(lambda tweet: len([word for word in str(tweet).lower().split() if word not in stop_words]))\n",
    "print(\"Number of words: \", df_words.sum())\n",
    "print(\"Number of stopwords: \", df_stopwords.sum())\n",
    "print(\"Number of non stopwords: \", df_non_stopwords.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def word_count(df):\n",
    "    word_cloud = WordCloud(max_words=200, background_color='white',stopwords=stop_words,colormap='rainbow',height=1000,width=700)\n",
    "    tweets = df.tweet.values\n",
    "    word_cloud.generate(str(tweets).lower())\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(word_cloud)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.set_figheight(10)\n",
    "    plt.show()\n",
    "\n",
    "word_count(df)"
   ]
  },
  {
   "source": [
    "## Data cleaning\n",
    "\n",
    "- There are a whole lot of strange abbreviations in tweets. \n",
    "- People always write pretty informally when sending tweets.\n",
    "- Of course, typos are huge problems, too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the lowercase \n",
    "df.tweet = df.tweet.str.lower()\n",
    "# abbreviation check list\n",
    "abbreviations = {\n",
    "        \"$\" : \" dollar \",\n",
    "        \"€\" : \" euro \",\n",
    "        \"4ao\" : \"for adults only\",\n",
    "        \"a.m\" : \"before midday\",\n",
    "        \"a3\" : \"anytime anywhere anyplace\",\n",
    "        \"aamof\" : \"as a matter of fact\",\n",
    "        \"acct\" : \"account\",\n",
    "        \"adih\" : \"another day in hell\",\n",
    "        \"afaic\" : \"as far as i am concerned\",\n",
    "        \"afaict\" : \"as far as i can tell\",\n",
    "        \"afaik\" : \"as far as i know\",\n",
    "        \"afair\" : \"as far as i remember\",\n",
    "        \"afk\" : \"away from keyboard\",\n",
    "        \"app\" : \"application\",\n",
    "        \"approx\" : \"approximately\",\n",
    "        \"apps\" : \"applications\",\n",
    "        \"asap\" : \"as soon as possible\",\n",
    "        \"asl\" : \"age, sex, location\",\n",
    "        \"atk\" : \"at the keyboard\",\n",
    "        \"ave.\" : \"avenue\",\n",
    "        \"aymm\" : \"are you my mother\",\n",
    "        \"ayor\" : \"at your own risk\", \n",
    "        \"b&b\" : \"bed and breakfast\",\n",
    "        \"b+b\" : \"bed and breakfast\",\n",
    "        \"b.c\" : \"before christ\",\n",
    "        \"b2b\" : \"business to business\",\n",
    "        \"b2c\" : \"business to customer\",\n",
    "        \"b4\" : \"before\",\n",
    "        \"b4n\" : \"bye for now\",\n",
    "        \"b@u\" : \"back at you\",\n",
    "        \"bae\" : \"before anyone else\",\n",
    "        \"bak\" : \"back at keyboard\",\n",
    "        \"bbbg\" : \"bye bye be good\",\n",
    "        \"bbc\" : \"british broadcasting corporation\",\n",
    "        \"bbias\" : \"be back in a second\",\n",
    "        \"bbl\" : \"be back later\",\n",
    "        \"bbs\" : \"be back soon\",\n",
    "        \"be4\" : \"before\",\n",
    "        \"bfn\" : \"bye for now\",\n",
    "        \"blvd\" : \"boulevard\",\n",
    "        \"bout\" : \"about\",\n",
    "        \"brb\" : \"be right back\",\n",
    "        \"bros\" : \"brothers\",\n",
    "        \"brt\" : \"be right there\",\n",
    "        \"bsaaw\" : \"big smile and a wink\",\n",
    "        \"btw\" : \"by the way\",\n",
    "        \"bwl\" : \"bursting with laughter\",\n",
    "        \"c/o\" : \"care of\",\n",
    "        \"cet\" : \"central european time\",\n",
    "        \"cf\" : \"compare\",\n",
    "        \"cia\" : \"central intelligence agency\",\n",
    "        \"csl\" : \"can not stop laughing\",\n",
    "        \"cu\" : \"see you\",\n",
    "        \"cul8r\" : \"see you later\",\n",
    "        \"cv\" : \"curriculum vitae\",\n",
    "        \"cwot\" : \"complete waste of time\",\n",
    "        \"cya\" : \"see you\",\n",
    "        \"cyt\" : \"see you tomorrow\",\n",
    "        \"dae\" : \"does anyone else\",\n",
    "        \"dbmib\" : \"do not bother me i am busy\",\n",
    "        \"diy\" : \"do it yourself\",\n",
    "        \"dm\" : \"direct message\",\n",
    "        \"dwh\" : \"during work hours\",\n",
    "        \"e123\" : \"easy as one two three\",\n",
    "        \"eet\" : \"eastern european time\",\n",
    "        \"eg\" : \"example\",\n",
    "        \"embm\" : \"early morning business meeting\",\n",
    "        \"encl\" : \"enclosed\",\n",
    "        \"encl.\" : \"enclosed\",\n",
    "        \"etc\" : \"and so on\",\n",
    "        \"faq\" : \"frequently asked questions\",\n",
    "        \"fawc\" : \"for anyone who cares\",\n",
    "        \"fb\" : \"facebook\",\n",
    "        \"fc\" : \"fingers crossed\",\n",
    "        \"fig\" : \"figure\",\n",
    "        \"fimh\" : \"forever in my heart\", \n",
    "        \"ft.\" : \"feet\",\n",
    "        \"ft\" : \"featuring\",\n",
    "        \"ftl\" : \"for the loss\",\n",
    "        \"ftw\" : \"for the win\",\n",
    "        \"fwiw\" : \"for what it is worth\",\n",
    "        \"fyi\" : \"for your information\",\n",
    "        \"g9\" : \"genius\",\n",
    "        \"gahoy\" : \"get a hold of yourself\",\n",
    "        \"gal\" : \"get a life\",\n",
    "        \"gcse\" : \"general certificate of secondary education\",\n",
    "        \"gfn\" : \"gone for now\",\n",
    "        \"gg\" : \"good game\",\n",
    "        \"gl\" : \"good luck\",\n",
    "        \"glhf\" : \"good luck have fun\",\n",
    "        \"gmt\" : \"greenwich mean time\",\n",
    "        \"gmta\" : \"great minds think alike\",\n",
    "        \"gn\" : \"good night\",\n",
    "        \"g.o.a.t\" : \"greatest of all time\",\n",
    "        \"goat\" : \"greatest of all time\",\n",
    "        \"goi\" : \"get over it\",\n",
    "        \"gps\" : \"global positioning system\",\n",
    "        \"gr8\" : \"great\",\n",
    "        \"gratz\" : \"congratulations\",\n",
    "        \"gyal\" : \"girl\",\n",
    "        \"h&c\" : \"hot and cold\",\n",
    "        \"hp\" : \"horsepower\",\n",
    "        \"hr\" : \"hour\",\n",
    "        \"hrh\" : \"his royal highness\",\n",
    "        \"ht\" : \"height\",\n",
    "        \"ibrb\" : \"i will be right back\",\n",
    "        \"ic\" : \"i see\",\n",
    "        \"icq\" : \"i seek you\",\n",
    "        \"icymi\" : \"in case you missed it\",\n",
    "        \"idc\" : \"i do not care\",\n",
    "        \"idgadf\" : \"i do not give a damn fuck\",\n",
    "        \"idgaf\" : \"i do not give a fuck\",\n",
    "        \"idk\" : \"i do not know\",\n",
    "        \"ie\" : \"that is\",\n",
    "        \"i.e\" : \"that is\",\n",
    "        \"ifyp\" : \"i feel your pain\",\n",
    "        \"ig\" : \"instagram\",\n",
    "        \"iirc\" : \"if i remember correctly\",\n",
    "        \"ilu\" : \"i love you\",\n",
    "        \"ily\" : \"i love you\",\n",
    "        \"imho\" : \"in my humble opinion\",\n",
    "        \"imo\" : \"in my opinion\",\n",
    "        \"imu\" : \"i miss you\",\n",
    "        \"iow\" : \"in other words\",\n",
    "        \"irl\" : \"in real life\",\n",
    "        \"j4f\" : \"just for fun\",\n",
    "        \"jic\" : \"just in case\",\n",
    "        \"jk\" : \"just kidding\",\n",
    "        \"jsyk\" : \"just so you know\",\n",
    "        \"l8r\" : \"later\",\n",
    "        \"lb\" : \"pound\",\n",
    "        \"lbs\" : \"pounds\",\n",
    "        \"ldr\" : \"long distance relationship\",\n",
    "        \"lmao\" : \"laugh my ass off\",\n",
    "        \"lmfao\" : \"laugh my fucking ass off\",\n",
    "        \"lol\" : \"laughing out loud\",\n",
    "        \"ltd\" : \"limited\",\n",
    "        \"ltns\" : \"long time no see\",\n",
    "        \"m8\" : \"mate\",\n",
    "        \"mf\" : \"motherfucker\",\n",
    "        \"mfs\" : \"motherfuckers\",\n",
    "        \"mfw\" : \"my face when\",\n",
    "        \"mofo\" : \"motherfucker\",\n",
    "        \"mph\" : \"miles per hour\",\n",
    "        \"mr\" : \"mister\",\n",
    "        \"mrw\" : \"my reaction when\",\n",
    "        \"ms\" : \"miss\",\n",
    "        \"mte\" : \"my thoughts exactly\",\n",
    "        \"nagi\" : \"not a good idea\",\n",
    "        \"nbc\" : \"national broadcasting company\",\n",
    "        \"nbd\" : \"not big deal\",\n",
    "        \"nfs\" : \"not for sale\",\n",
    "        \"ngl\" : \"not going to lie\",\n",
    "        \"nhs\" : \"national health service\",\n",
    "        \"nrn\" : \"no reply necessary\",\n",
    "        \"nsfl\" : \"not safe for life\",\n",
    "        \"nsfw\" : \"not safe for work\",\n",
    "        \"nth\" : \"nice to have\",\n",
    "        \"nvr\" : \"never\",\n",
    "        \"nyc\" : \"new york city\",\n",
    "        \"oc\" : \"original content\",\n",
    "        \"og\" : \"original\",\n",
    "        \"ohp\" : \"overhead projector\",\n",
    "        \"oic\" : \"oh i see\",\n",
    "        \"omdb\" : \"over my dead body\",\n",
    "        \"omg\" : \"oh my god\",\n",
    "        \"omw\" : \"on my way\",\n",
    "        \"p.a\" : \"per annum\",\n",
    "        \"p.m\" : \"after midday\",\n",
    "        \"pm\" : \"prime minister\",\n",
    "        \"poc\" : \"people of color\",\n",
    "        \"pov\" : \"point of view\",\n",
    "        \"pp\" : \"pages\",\n",
    "        \"ppl\" : \"people\",\n",
    "        \"prw\" : \"parents are watching\",\n",
    "        \"ps\" : \"postscript\",\n",
    "        \"pt\" : \"point\",\n",
    "        \"ptb\" : \"please text back\",\n",
    "        \"pto\" : \"please turn over\",\n",
    "        \"qpsa\" : \"what happens\", \n",
    "        \"ratchet\" : \"rude\",\n",
    "        \"rbtl\" : \"read between the lines\",\n",
    "        \"rlrt\" : \"real life retweet\", \n",
    "        \"rofl\" : \"rolling on the floor laughing\",\n",
    "        \"roflol\" : \"rolling on the floor laughing out loud\",\n",
    "        \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n",
    "        \"rt\" : \"retweet\",\n",
    "        \"ruok\" : \"are you ok\",\n",
    "        \"sfw\" : \"safe for work\",\n",
    "        \"sk8\" : \"skate\",\n",
    "        \"smh\" : \"shake my head\",\n",
    "        \"sq\" : \"square\",\n",
    "        \"srsly\" : \"seriously\", \n",
    "        \"ssdd\" : \"same stuff different day\",\n",
    "        \"tbh\" : \"to be honest\",\n",
    "        \"tbs\" : \"tablespooful\",\n",
    "        \"tbsp\" : \"tablespooful\",\n",
    "        \"tfw\" : \"that feeling when\",\n",
    "        \"thks\" : \"thank you\",\n",
    "        \"tho\" : \"though\",\n",
    "        \"thx\" : \"thank you\",\n",
    "        \"tia\" : \"thanks in advance\",\n",
    "        \"til\" : \"today i learned\",\n",
    "        \"tl;dr\" : \"too long i did not read\",\n",
    "        \"tldr\" : \"too long i did not read\",\n",
    "        \"tmb\" : \"tweet me back\",\n",
    "        \"tntl\" : \"trying not to laugh\",\n",
    "        \"ttyl\" : \"talk to you later\",\n",
    "        \"u\" : \"you\",\n",
    "        \"u2\" : \"you too\",\n",
    "        \"u4e\" : \"yours for ever\",\n",
    "        \"utc\" : \"coordinated universal time\",\n",
    "        \"w/\" : \"with\",\n",
    "        \"w/o\" : \"without\",\n",
    "        \"w8\" : \"wait\",\n",
    "        \"wassup\" : \"what is up\",\n",
    "        \"wb\" : \"welcome back\",\n",
    "        \"wtf\" : \"what the fuck\",\n",
    "        \"wtg\" : \"way to go\",\n",
    "        \"wtpa\" : \"where the party at\",\n",
    "        \"wuf\" : \"where are you from\",\n",
    "        \"wuzup\" : \"what is up\",\n",
    "        \"wywh\" : \"wish you were here\",\n",
    "        \"yd\" : \"yard\",\n",
    "        \"ygtr\" : \"you got that right\",\n",
    "        \"ynk\" : \"you never know\",\n",
    "        \"zzz\" : \"sleeping bored and tired\"\n",
    "    }\n",
    "\n",
    "def get_tokens(tweet):\n",
    "    # remove special chars\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÀ\", \"\", tweet)\n",
    "    # remove contractions\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"that\\x89Ûªs\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)  \n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªre\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"i am\", tweet)\n",
    "    tweet = re.sub(r\"i\\x89Ûªm\", \"i am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"here's\", \"here is\", tweet)\n",
    "    tweet = re.sub(r\"here\\x89Ûªs\", \"here is\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"i have\", tweet)\n",
    "    tweet = re.sub(r\"i\\x89Ûªve\", \"i have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"i would\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"i will\", tweet)\n",
    "    tweet = re.sub(r\"^ill$\", \"i will\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)    \n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)    \n",
    "    tweet = re.sub(r\"you'd\", \"you would\", tweet)\n",
    "    tweet = re.sub(r\"could've\", \"could have\", tweet)\n",
    "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
    "    tweet = re.sub(r\"some1\", \"someone\", tweet)\n",
    "    tweet = re.sub(r\"yrs\", \"years\", tweet)\n",
    "    tweet = re.sub(r\"hrs\", \"hours\", tweet)\n",
    "    tweet = re.sub(r\"2morow|2moro\", \"tomorrow\", tweet)\n",
    "    tweet = re.sub(r\"2day\", \"today\", tweet)\n",
    "    tweet = re.sub(r\"4got|4gotten\", \"forget\", tweet)\n",
    "    tweet = re.sub(r\"b-day|bday\", \"b-day\", tweet)\n",
    "    tweet = re.sub(r\"mother's\", \"mother\", tweet)\n",
    "    tweet = re.sub(r\"mom's\", \"mom\", tweet)\n",
    "    tweet = re.sub(r\"dad's\", \"dad\", tweet)\n",
    "    tweet = re.sub(r\"^[h|a]+$\", \"haha\", tweet)\n",
    "    tweet = re.sub(r\"lmao|lolz|rofl\", \"lol\", tweet)\n",
    "    tweet = re.sub(r\"thanx|thnx|thx\", \"thanks\", tweet)\n",
    "    tweet = re.sub(r'all[l]+', \"all\", tweet)\n",
    "    tweet = re.sub(r'so[o]+', \"so\", tweet)\n",
    "    tweet = re.sub(r'why[y]+', \"why\", tweet)\n",
    "    tweet = re.sub(r'way[y]+', \"way\", tweet)\n",
    "    tweet = re.sub(r'will[l]+', \"will\", tweet)\n",
    "    tweet = re.sub(r'oo[o]+h', \"ooh\", tweet)\n",
    "    tweet = re.sub(r'hey[y]+', \"hey\", tweet)\n",
    "    tweet = re.sub(r\"boo[o]+m\", \"boom\", tweet)\n",
    "    tweet = re.sub(r\"co[o]+ld\", \"cold\", tweet)\n",
    "    tweet = re.sub(r\"goo[o]+d\", \"good\", tweet)\n",
    "    # deal with some abbreviations\n",
    "    words = tweet.split()\n",
    "    tweet = ' '.join([abbreviations[word] if word in abbreviations.keys() else word for word in words])\n",
    "    # character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    # typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"usagov\", \"usa government\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"trfc\", \"traffic\", tweet)\n",
    "    # remove urls\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    # remove mentions\n",
    "    tweet = re.sub(r'^@[0-9a-zA-Z_]+', \"\", tweet)\n",
    "    # words with punctuations and special characters\n",
    "    for punc in string.punctuation:\n",
    "        tweet = tweet.replace(punc, '')\n",
    "    # ... and ..\n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ')\n",
    "    # get the tokens\n",
    "    tweet = [word for word in str(tweet).split() if word not in stop_words]\n",
    "    tweet = ' '.join(tweet)\n",
    "    return tweet\n",
    "\n",
    "df['tokens'] = df.tweet.apply(lambda tweet: get_tokens(tweet))\n",
    "print(\"done\")"
   ]
  },
  {
   "source": [
    "- Write the results to the target file.\n",
    "- And check the words' frequency after cleaning."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['sentiment', 'tokens']]\n",
    "df.rename(columns={'tokens': 'tweet'}, inplace=True)\n",
    "df.to_csv(\"D:/CodeBase-User/VScode-workspace/BERT-Notebooks/Dataset/noemoticon-cleaned-1000.csv\")\n",
    "word_count(df)"
   ]
  },
  {
   "source": [
    "## Model building (Pytorch & transformers)\n",
    "\n",
    "- Now using the pre-trained transformers models from the huggingface with Pytorch.\n",
    "- I choose to use the 'bert-base-uncased' model\n",
    "- The dataset only has two classes: positive & negative."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "max_seq_length = 128\n",
    "num_class = 1\n",
    "batch_size = 32\n",
    "epoch = 10\n",
    "\n",
    "bert_model_path = \"E:/PreTrainedModels/Huggingface/bert-base-uncased\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "df = pd.read_csv(\"D:/CodeBase-User/VScode-workspace/BERT-Notebooks/Dataset/noemoticon-cleaned-1000.csv\")\n",
    "df.head()"
   ]
  },
  {
   "source": [
    "- Implement a method to tokenize the tweets.\n",
    "- Not directly using the batch tokenize function in the transformer's pre-built tokenizer: Show the structure of each input parts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_tweet_format(df, tokenizer, max_seq_length):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_labels = []\n",
    "    for tweet in df.tweet:\n",
    "        tweet = tokenizer.tokenize(str(tweet))\n",
    "        # No longer the the max length limitation\n",
    "        tweet = tweet[:max_seq_length - 2]\n",
    "        # '[CLS], token_1, ~, token_n, [SEP]'\n",
    "        input_sequence = ['[CLS]'] + tweet + ['[SEP]']\n",
    "        pad_len = max_seq_length - len(input_sequence)\n",
    "        # Get the id of each token\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        # Now length = max_seq_length\n",
    "        tokens += [0] * pad_len\n",
    "        # All places that have contents is marked as 1, otherwise 0\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "    for label in df.sentiment:\n",
    "        all_labels.append([label])\n",
    "    return torch.tensor(all_tokens, dtype=torch.long), torch.tensor(all_masks, dtype=torch.long), torch.tensor(all_labels, dtype=torch.float)"
   ]
  },
  {
   "source": [
    "- Now I load the model from the huggingface repo. \n",
    "- It can be loaded locally once downloaded.\n",
    "- Load the random sampler for batch training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained(bert_model_path)\n",
    "bert_config = BertConfig.from_pretrained(bert_model_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_path)\n",
    "\n",
    "all_input_ids, all_input_mask, all_labels = covert_tweet_format(df, tokenizer, max_seq_length)\n",
    "\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"
   ]
  },
  {
   "source": [
    "- Define the customerized extra layers of the model \n",
    "- Using the pretrained bert model as my bert layer and freeze its weight\n",
    "- The learning rate of bias and layer norm layers will not decay during training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier(torch.nn.Module):\n",
    "    def __init__(self, bert_model, bert_config, num_class):\n",
    "        super(TweetClassifier, self).__init__()\n",
    "        self.bert_layer = bert_model\n",
    "        self.output = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(0.3),\n",
    "            torch.nn.Linear(bert_config.hidden_size, bert_config.hidden_size//2),\n",
    "            torch.nn.Linear(bert_config.hidden_size//2, bert_config.hidden_size),\n",
    "            torch.nn.Linear(bert_config.hidden_size, num_class),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input_ids, attn_mask):\n",
    "        bert_out = self.bert_layer(input_ids, attention_mask=attn_mask)\n",
    "        out = self.output(bert_out[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_classifier = TweetClassifier(bert_model=bert_model, bert_config=bert_config, num_class=num_class)\n",
    "tweet_classifier = tweet_classifier.to(device)\n",
    "# Not change the params in the pre-trained model\n",
    "freeze_param = ['bert_layer']\n",
    "for n, p in tweet_classifier.named_parameters():\n",
    "    if any(nd in n for nd in freeze_param):\n",
    "        p.requires_grad = False\n",
    "# Adjust the learning rate decay for some layers\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in tweet_classifier.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in tweet_classifier.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "tweet_classifier.train()"
   ]
  },
  {
   "source": [
    "- Finally, define the trainiing process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(epoch):\n",
    "    epoch_loss = 0\n",
    "    for batch, (token_ids, attn_mask, label) in enumerate(train_dataloader):\n",
    "        # keep all the parameters in the same device\n",
    "        token_ids = token_ids.to(device)\n",
    "        attn_mask = attn_mask.to(device)\n",
    "        label = label.to(device)\n",
    "        # the output will be in the same device with the model\n",
    "        outputs = tweet_classifier(token_ids, attn_mask)\n",
    "        loss = F.binary_cross_entropy(outputs, label)\n",
    "        # do the backprop and update the parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.cpu().data.numpy()\n",
    "        if batch % 30 == 0:\n",
    "                print(\"Current batch loss: \", loss.cpu().data.numpy())\n",
    "    print(\"Now epoch total loss: \", epoch_loss)"
   ]
  },
  {
   "source": [
    "## Model building (inherit from BertPreTrainedModel)\n",
    "\n",
    "- Another example of using pre-trained models from huggingface, using the transformers with Pytorch.\n",
    "- This time I inherit the `BertPreTrainedModel` class and use `from_pretrained()` method to create model.\n",
    "- What's more, I directly use the tokenizer from the hugginface and add k-fold training strategy. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AdamW, BertPreTrainedModel\n",
    "\n",
    "\n",
    "max_seq_length = 128\n",
    "num_class = 1\n",
    "batch_size = 32\n",
    "k = 3\n",
    "seed = 731\n",
    "\n",
    "bert_path = \"E:/PreTrainedModels/Huggingface/bert-base-uncased\"\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "df_train = pd.read_csv(\"D:/CodeBase-User/VScode-workspace/BERT-Notebooks/Dataset/noemoticon-cleaned-1000.csv\")\n",
    "df_train.reset_index()\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetClassifier(BertPreTrainedModel):\n",
    "    def __init__(self, bert_model, bert_config, num_class):\n",
    "        super(TweetClassifier, self).__init__(bert_config)\n",
    "        self.bert_layer = BertModel.from_pretrained(bert_model, config=bert_config)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.output = torch.nn.Sequential(            \n",
    "            torch.nn.Linear(bert_config.hidden_size, bert_config.hidden_size//2),\n",
    "            torch.nn.Linear(bert_config.hidden_size//2, bert_config.hidden_size),\n",
    "            torch.nn.Linear(bert_config.hidden_size, num_class),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        bert_out = self.bert_layer(input_ids, attention_mask=attn_mask, token_type_ids=token_type_ids)\n",
    "        output = self.dropout(bert_out[-1])\n",
    "        output = self.output(output)\n",
    "        return output"
   ]
  },
  {
   "source": [
    "- Directly use the tokenizer given by huggingface.\n",
    "- Also, another hepler function to get rid of the unexisting tweets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covert_tweet_format(tweets, sentiments, tokenizer, max_seq_length):\n",
    "    tweet_list = []\n",
    "    for tweet in tweets:\n",
    "        tweet_list.append(str(tweet))\n",
    "    encoded_inputs = tokenizer(tweet_list, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors=\"pt\")\n",
    "    all_labels = []\n",
    "    for label in sentiments:\n",
    "        all_labels.append([label])\n",
    "    return encoded_inputs, torch.tensor(all_labels, dtype=torch.float)"
   ]
  },
  {
   "source": [
    "- Define the training set and the optimizer\n",
    "- Load the pretrained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "k_fold = StratifiedKFold(n_splits=k, random_state=seed, shuffle=True)\n",
    "bert_config = BertConfig.from_pretrained(bert_path)\n",
    "tweet_classifier = TweetClassifier(bert_path, bert_config, num_class)\n",
    "tweet_classifier = tweet_classifier.to(device)\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in tweet_classifier.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in tweet_classifier.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "\n",
    "tweet_classifier.train()"
   ]
  },
  {
   "source": [
    "- Now train the Bert model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (trn_idx, val_idx) in enumerate(k_fold.split(df_train.tweet, df.sentiment)):\n",
    "    trn_idx = clean_unexist_idx(trn_idx)\n",
    "    encoded_inputs, labels = covert_tweet_format(df.loc[trn_idx, 'tweet'], df.loc[trn_idx, 'sentiment'], tokenizer, max_seq_length)\n",
    "    train_data = TensorDataset(encoded_inputs['input_ids'], encoded_inputs['attention_mask'], encoded_inputs['token_type_ids'], labels)\n",
    "    train_sampler = for fold, (trn_idx, val_idx) in enumerate(k_fold.split(df_train.tweet, df_train.sentiment)):\n",
    "    encoded_inputs, labels = covert_tweet_format(df_train.loc[trn_idx, 'tweet'], df_train.loc[trn_idx, 'sentiment'], tokenizer, max_seq_length)\n",
    "    train_data = TensorDataset(encoded_inputs['input_ids'], encoded_inputs['attention_mask'], encoded_inputs['token_type_ids'], labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    for epoch in range(epoch_num):\n",
    "        epoch_loss = 0\n",
    "        for batch, (token_ids, attn_mask, token_types, labels) in enumerate(train_dataloader):\n",
    "            token_ids = token_ids.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            token_types = token_types.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = tweet_classifier(token_ids, attn_mask, token_types)\n",
    "            loss = F.binary_cross_entropy(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.cpu().data.numpy()\n",
    "            if batch % 30 == 0:\n",
    "                print(\"Current batch loss: \", loss.cpu().data.numpy())\n",
    "        print(\"Now epoch: \", epoch + 1, \" current loss: \", epoch_loss)"
   ]
  }
 ]
}