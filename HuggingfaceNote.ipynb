{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('envPytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "82c25c09a0a8ff8dd25a51ab110a5b27617daf1cc0c39e1b255c757f044d7c3e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Simple note on how to use pre-trained models from hugginface\n",
    "\n",
    "### Tokenizer\n",
    "\n",
    "- Can tokenize one or a batch of sentences at one time"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 624/624 [00:00<00:00, 626kB/s]\n",
      "Downloading: 100%|██████████| 110k/110k [00:00<00:00, 147kB/s] \n",
      "Downloading: 100%|██████████| 269k/269k [00:01<00:00, 265kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids  :  [101, 2769, 2218, 2682, 1726, 1168, 7411, 6498, 2218, 679, 2802, 749, 102]\ntoken_type_ids  :  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nattention_mask  :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "input_tokens = tokenizer(\"我就想回到雀豪就不打了\")\n",
    "for key in input_tokens:\n",
    "    print(key, \" : \", input_tokens[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] 我 就 想 回 到 雀 豪 就 不 打 了 [SEP]'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "tokenizer.decode(input_tokens['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids  :  tensor([[ 101, 1372, 6878, 1168,  749, 1908, 6438, 3322, 1469, 7649,  102],\n        [ 101, 5018,  753,  702, 1962, 1008, 3221,  102,    0,    0,    0],\n        [ 101, 2769, 2798, 2828, 6825, 3168, 4500,  749,  102,    0,    0],\n        [ 101, 8131, 2231, 6158, 1730, 4127,  102,    0,    0,    0,    0]])\ntoken_type_ids  :  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\nattention_mask  :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\"只遇到了复读机和饭\",\n",
    "                   \"第二个好像是\",\n",
    "                   \"我才把连斩用了\",\n",
    "                   \"19层被团灭\"]\n",
    "encoded_batch_inputs = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "for key in encoded_batch_inputs:\n",
    "    print(key, \" : \", encoded_batch_inputs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids  :  [101, 1525, 702, 3221, 1475, 1475, 102, 782, 5102, 4638, 3315, 6574, 102]\ntoken_type_ids  :  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\nattention_mask  :  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "qes_input = \"哪个是咕咕\"\n",
    "ans_input = \"人类的本质\"\n",
    "encoded_qa = tokenizer(qes_input, ans_input)\n",
    "for key in encoded_qa:\n",
    "    print(key, \" : \", encoded_qa[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] 哪 个 是 咕 咕 [SEP] 人 类 的 本 质 [SEP]'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "tokenizer.decode(encoded_qa['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_ids  :  tensor([[ 101, 1525,  702, 3221, 1475, 1475,  102,  782, 5102, 4638, 3315, 6574,\n          102,    0,    0,    0,    0,    0,    0,    0],\n        [ 101, 1961, 2544, 1300, 1726, 2418,  749, 1091, 1567,  102, 2218, 7350,\n         2349, 7350, 2349, 7350, 2349,  102,    0,    0]])\ntoken_type_ids  :  tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\nattention_mask  :  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "qes_batch = [\"哪个是咕咕\",\n",
    "             \"她微博回应了写啥\"]\n",
    "ans_batch = [\"人类的本质\",\n",
    "             \"就阿巴阿巴阿巴\"]\n",
    "encoded_batch_qa = tokenizer(qes_batch, ans_batch, padding='max_length', max_length=20, truncation=True, return_tensors=\"pt\")\n",
    "for key in encoded_batch_qa:\n",
    "    print(key, \" : \", encoded_batch_qa[key])"
   ]
  },
  {
   "source": [
    "### How to load a pre-trained model and use it"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import BertTokenizer\n",
    "from torch.nn import functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-chinese')\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "batch_sentences = [\"只遇到了复读机和饭\",\n",
    "                   \"第二个好像是\",\n",
    "                   \"我才把连斩用了\",\n",
    "                   \"我们进门挂机1分钟然后直奔领奖台\",\n",
    "                   \"钓鱼执法是吧\",\n",
    "                   \"19层被团灭\"]\n",
    "labels = torch.Tensor([1, 0, 0, 1, 0, 0])\n",
    "\n",
    "encodings = tokenizer(batch_sentences, padding='max_length', max_length=20, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids']\n",
    "input_mask = encodings['attention_mask']\n",
    "\n",
    "outputs = model(input_ids, attention_mask=input_mask)\n",
    "# loss = outputs.loss\n",
    "loss = F.cross_entropy(outputs.logits, labels)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./my_mrpc_model/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./my_mrpc_model/', from_tf=True)"
   ]
  }
 ]
}