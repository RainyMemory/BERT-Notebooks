{"metadata":{"kernelspec":{"name":"python368jvsc74a57bd082c25c09a0a8ff8dd25a51ab110a5b27617daf1cc0c39e1b255c757f044d7c3e","display_name":"Python 3.6.8 64-bit ('envPytorch': conda)"},"language_info":{"name":"python","version":"3.6.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":["# Playing with real & fake news data from Kaggle\n","\n","- 承接[newsExplore](../DataExploring/newsExplore.ipynb)中内容\n","- 进行一部分特征工程，依赖其结果尝试一些简单模型\n","- 使用GRU与BERT，根据新闻正文内容进行预测\n","\n","## 特征提取"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/fake-and-real-news-dataset/True.csv\n/kaggle/input/fake-and-real-news-dataset/Fake.csv\n","output_type":"stream"}]},{"cell_type":"code","source":["real_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/True.csv')\n","fake_news = pd.read_csv('/kaggle/input/fake-and-real-news-dataset/Fake.csv')"],"metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"source":["### 移除Real News特有的，指名新闻消息来源的前置文本。\n","- 如：`WASHINGTON (Reuters) - `"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["import re \n","def removePrefix(text):\n","    pattern = r\"^([A-Z]).*?-\\s\"\n","    text = re.sub(pattern, '', text)\n","    return text\n","\n","real_news.text = real_news.text.apply(lambda x : removePrefix(x))\n","real_news.text[1]"],"metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'Transgender people will be allowed for the first time to enlist in the U.S. military starting on Monday as ordered by federal courts, the Pentagon said on Friday, after President Donald Trump’s administration decided not to appeal rulings that blocked his transgender ban. Two federal appeals courts, one in Washington and one in Virginia, last week rejected the administration’s request to put on hold orders by lower court judges requiring the military to begin accepting transgender recruits on Jan. 1. A Justice Department official said the administration will not challenge those rulings. “The Department of Defense has announced that it will be releasing an independent study of these issues in the coming weeks. So rather than litigate this interim appeal before that occurs, the administration has decided to wait for DOD’s study and will continue to defend the president’s lawful authority in District Court in the meantime,” the official said, speaking on condition of anonymity. In September, the Pentagon said it had created a panel of senior officials to study how to implement a directive by Trump to prohibit transgender individuals from serving. The Defense Department has until Feb. 21 to submit a plan to Trump. Lawyers representing currently-serving transgender service members and aspiring recruits said they had expected the administration to appeal the rulings to the conservative-majority Supreme Court, but were hoping that would not happen. Pentagon spokeswoman Heather Babb said in a statement: “As mandated by court order, the Department of Defense is prepared to begin accessing transgender applicants for military service Jan. 1. All applicants must meet all accession standards.” Jennifer Levi, a lawyer with gay, lesbian and transgender advocacy group GLAD, called the decision not to appeal “great news.” “I’m hoping it means the government has come to see that there is no way to justify a ban and that it’s not good for the military or our country,” Levi said. Both GLAD and the American Civil Liberties Union represent plaintiffs in the lawsuits filed against the administration. In a move that appealed to his hard-line conservative supporters, Trump announced in July that he would prohibit transgender people from serving in the military, reversing Democratic President Barack Obama’s policy of accepting them. Trump said on Twitter at the time that the military “cannot be burdened with the tremendous medical costs and disruption that transgender in the military would entail.” Four federal judges - in Baltimore, Washington, D.C., Seattle and Riverside, California - have issued rulings blocking Trump’s ban while legal challenges to the Republican president’s policy proceed. The judges said the ban would likely violate the right under the U.S. Constitution to equal protection under the law. The Pentagon on Dec. 8 issued guidelines to recruitment personnel in order to enlist transgender applicants by Jan. 1. The memo outlined medical requirements and specified how the applicants’ sex would be identified and even which undergarments they would wear. The Trump administration previously said in legal papers that the armed forces were not prepared to train thousands of personnel on the medical standards needed to process transgender applicants and might have to accept “some individuals who are not medically fit for service.” The Obama administration had set a deadline of July 1, 2017, to begin accepting transgender recruits. But Trump’s defense secretary, James Mattis, postponed that date to Jan. 1, 2018, which the president’s ban then put off indefinitely. Trump has taken other steps aimed at rolling back transgender rights. In October, his administration said a federal law banning gender-based workplace discrimination does not protect transgender employees, reversing another Obama-era position. In February, Trump rescinded guidance issued by the Obama administration saying that public schools should allow transgender students to use the restroom that corresponds to their gender identity. '"},"metadata":{}}]},{"source":["### 虽然info中给出数据各列都未非空，实际上有空文本存在（仅含空格），对其进行清理\n","- 同时还要注意清理重复新闻\n","- 注意清理后真假新闻两类数目的不平衡会进一步扩大"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["real_news['valid'] = 1\n","fake_news['valid'] = 0\n","\n","news_source = pd.concat([real_news, fake_news], axis=0)\n","news_source = news_source[news_source[['text', 'title', 'date']].duplicated() == False]\n","news_source.text = news_source.text.apply(lambda x: np.nan if len(x.strip()) < 1 else x)\n","news_source = news_source.dropna()\n","news_source.drop(columns=['subject'], inplace=True)\n","news_source.duplicated().sum()"],"metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":["news_source.to_csv('./Source.csv')"],"metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import re\n","import nltk\n","import string\n","import pandas as pd\n","\n","news_source = pd.read_csv('./Source.csv')\n","stop_words = set(stopwords.words('english'))\n","punctuations = set(string.punctuation)"],"metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"source":["### 将两类新闻中前十的高频词取出，由之前的分析可看出，两类新闻有着不同的高频话题倾向\n","- 故将各条新闻中所含的，此两类高频词的数目作为特征提出"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def token_freq(df, feature, valid):\n","    tar_texts = df[df.valid == valid][feature].values\n","    texts = ' '.join(tar_texts).lower()\n","    tokens = ''.join(char for char in texts if char not in punctuations).split()\n","    tokens_cleaned = [word for word in tokens if word not in stop_words]\n","    return pd.DataFrame(nltk.FreqDist(tokens_cleaned).most_common(10))[0]\n","\n","real_title_freq = token_freq(news_source, 'title', 1)\n","real_text_freq = token_freq(news_source, 'text', 1)\n","fake_title_freq = token_freq(news_source, 'title', 0)\n","fake_text_freq = token_freq(news_source, 'text', 0)"],"metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def count_freq_token(text, freq_df):\n","    text = text.lower()\n","    tokens = ''.join(char for char in text if char not in punctuations).split()\n","    count = 0\n","    for token in tokens:\n","        if token in freq_df.values:\n","            count += 1\n","    return count"],"metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["news_source['fake_title_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, fake_title_freq))\n","news_source['read_title_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, real_title_freq))\n","news_source['fake_text_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, fake_text_freq))\n","news_source['read_text_token_freq'] = news_source.title.apply(lambda x : count_freq_token(x, real_text_freq))"],"metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"source":["### 已知假新闻中会使用大量带有感情色彩的符号来增强语气，故将其数目作为特征提出。"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def countPunctuation(text):\n","    ques = re.subn(r\"\\?\", \"\", text)[1]\n","    exclam = re.subn(r\"\\!\", \"\", text)[1]\n","    return ques, exclam"],"metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["news_source[['title_ques_num', 'title_exclam_num']] = news_source.title.apply(lambda x : pd.Series(countPunctuation(x)))\n","news_source[['text_ques_num', 'text_exclam_num']] = news_source.text.apply(lambda x : pd.Series(countPunctuation(x)))"],"metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"source":["### 假新闻和真新闻在平均长度上是有区别的，故将几个长度作为特征去除"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["news_source['title_len'] = news_source.title.apply(lambda x : len(x))\n","news_source['title_ratio'] = news_source.text.apply(lambda x : len(x))\n","news_source['title_ratio'] = news_source['title_len'] / news_source['title_ratio']"],"metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["news_source.to_csv('./NewsAna.csv')"],"metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["news_source.corr().valid"],"metadata":{"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Unnamed: 0               0.120031\nvalid                    1.000000\nfake_title_token_freq   -0.473264\nread_title_token_freq    0.194594\nfake_text_token_freq    -0.061342\nread_text_token_freq     0.064952\ntitle_ques_num          -0.144762\ntitle_exclam_num        -0.238136\ntext_ques_num           -0.324997\ntext_exclam_num         -0.237945\ntitle_len               -0.595802\ntitle_ratio             -0.066529\nName: valid, dtype: float64"},"metadata":{}}]},{"source":["## 随机打乱，开始利用简单模型建模拟合"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["import pandas as pd\n","news_source = pd.read_csv('./NewsAna.csv')\n","news_source.drop(columns=['Unnamed: 0', 'title', 'text', 'date'], inplace=True)"],"metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["news_source.reset_index()\n","news_source = news_source.sample(frac=1.)"],"metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier\n","\n","train_set, test_set = train_test_split(news_source, test_size=0.2, random_state=7)\n","train_x = train_set[['fake_title_token_freq', 'read_title_token_freq', 'fake_text_token_freq', \n","                 'read_text_token_freq', 'title_ques_num', 'title_exclam_num', 'text_ques_num', \n","                 'text_exclam_num', 'title_len', 'title_ratio']]\n","train_y = train_set['valid']\n","test_x = test_set[['fake_title_token_freq', 'read_title_token_freq', 'fake_text_token_freq', \n","                 'read_text_token_freq', 'title_ques_num', 'title_exclam_num', 'text_ques_num', \n","                 'text_exclam_num', 'title_len', 'title_ratio']]\n","test_y = test_set['valid']"],"metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def model_report(model, tar_x, tar_y):\n","    pred = model.predict(tar_x)\n","    f1 = f1_score(tar_y, pred)\n","    print(\"f1-score: \", f1)\n","    acc = accuracy_score(tar_y, pred)\n","    print(\"accuracy: \", acc)\n","    cm = confusion_matrix(tar_y, pred)\n","    print(\"confusion matrix:\\n\",cm)"],"metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["randomForest = RandomForestClassifier(random_state=7)\n","randomForest.fit(train_x, train_y)"],"metadata":{"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(random_state=7)"},"metadata":{}}]},{"cell_type":"code","source":["print(\"RF on training set:\")\n","model_report(randomForest, train_x, train_y)\n","print(\"\\nRF on testing set:\")\n","model_report(randomForest, test_x, test_y)"],"metadata":{"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"RF on training set:\nf1-score:  0.9993524459881086\naccuracy:  0.999288693459213\nconfusion matrix:\n [[13931    21]\n [    1 16976]]\n\nRF on testing set:\nf1-score:  0.926496530636246\naccuracy:  0.9191775507564981\nconfusion matrix:\n [[3169  343]\n [ 282 3939]]\n","output_type":"stream"}]},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","\n","decisionTree = DecisionTreeClassifier()\n","decisionTree.fit(train_x, train_y)"],"metadata":{"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"DecisionTreeClassifier()"},"metadata":{}}]},{"cell_type":"code","source":["print(\"tree on training set:\")\n","model_report(decisionTree, train_x, train_y)\n","print(\"\\ntree on testing set:\")\n","model_report(decisionTree, test_x, test_y)"],"metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"tree on training set:\nf1-score:  0.9993517208863745\naccuracy:  0.999288693459213\nconfusion matrix:\n [[13950     2]\n [   20 16957]]\n\ntree on testing set:\nf1-score:  0.8997046662728885\naccuracy:  0.8902107849476271\nconfusion matrix:\n [[3076  436]\n [ 413 3808]]\n","output_type":"stream"}]},{"cell_type":"code","source":["from sklearn.svm import LinearSVC\n","\n","linearSVC = LinearSVC(max_iter=5000, penalty='l2')\n","linearSVC.fit(train_x, train_y)"],"metadata":{"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"LinearSVC(max_iter=5000)"},"metadata":{}}]},{"cell_type":"code","source":["print(\"LinearSVC on training set:\")\n","model_report(linearSVC, train_x, train_y)\n","print(\"\\nLinearSVC on testing set:\")\n","model_report(linearSVC, test_x, test_y)"],"metadata":{"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"LinearSVC on training set:\nf1-score:  0.9178554141396885\naccuracy:  0.9067218468104368\nconfusion matrix:\n [[11926  2026]\n [  859 16118]]\n\nLinearSVC on testing set:\nf1-score:  0.9197431781701444\naccuracy:  0.9094788568472779\nconfusion matrix:\n [[3022  490]\n [ 210 4011]]\n","output_type":"stream"}]},{"source":["- 可以看出，通过数据分析提取出的部分特征，已经支持简单模型在测试集上达到90%左右的准确率和F1得分\n","- 可见真假新闻还是有相当多差异的，通过经典的NLP特征工程来分辨它们也能有不错的效果\n","- 加入更多Info Retrieve与NLP的特征应当可以进一步增强表现，如利用tf-idf值来提取标题关键词，加入词干词根提取工程等方式\n","\n","## 利用预训练BERT进行真假判定"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["from transformers import AutoModel, AutoConfig, AutoTokenizer, AdamW\n","from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler\n","import torch\n","\n","batch_size = 64\n","epoch_num = 4\n","max_seq_length = 128\n","\n","news_source = pd.read_csv('./NewsAna.csv')\n","news_source.reset_index()\n","news_source = news_source.sample(frac=1.)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","bert_model_path = \"bert-base-uncased\""],"metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"source":["### 定义模型，将输出维度调整到最终判定需要的维度"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["class NewsModel(torch.nn.Module):\n","    def __init__(self, bert_model=bert_model_path, num_class=1, add_feature_num=0):\n","        super(NewsModel, self).__init__()\n","        # 加载预训练模型(from huggingface)\n","        self.bert_layer = AutoModel.from_pretrained(pretrained_model_name_or_path=bert_model)\n","        # 或许可以补入一点特征数据，来求取最终结果\n","        self.bert_config = AutoConfig.from_pretrained(pretrained_model_name_or_path=bert_model)\n","        self.mid_dim = self.bert_config.hidden_size + add_feature_num\n","        # 进行最终分类\n","        self.output = torch.nn.Sequential(\n","            torch.nn.Linear(self.mid_dim, self.mid_dim//2),\n","            torch.nn.LeakyReLU(),\n","            torch.nn.Linear(self.mid_dim//2, self.mid_dim),\n","            torch.nn.LeakyReLU(),\n","            torch.nn.Linear(self.mid_dim, num_class),\n","            torch.nn.Sigmoid()\n","        )\n","    def forward(self, input_ids, attn_mask=None, add_features=[]):\n","        bert_out = self.bert_layer(input_ids=input_ids, attention_mask=attn_mask)[1]\n","        # 加入特征也应当为：[batch_size, add_feature_num]的形状\n","        if len(add_features) > 0:\n","            add_features = torch.tensor(add_features, dtype=torch.float)\n","            if add_features.shape[0] == bert_out.shape[0]:\n","                bert_out = torch.cat(bert_out, add_features)\n","        output = self.output(bert_out)\n","        return output"],"metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"source":["### 以Tensor的格式给出tokenize后的文本\n","- huggingface的tokenizer工作方式可以[看看这个](./HuggingfaceNote.ipynb)\n","- 以Tensor格式返回的好处就是能直接塞进TensorDataset中了，不需要再继承一个Dataset类"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["def covertTokenFormat(df, bert_model_path, max_seq_len):\n","    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=bert_model_path)\n","    titles = df.text.tolist()\n","    title_tokens = tokenizer(titles, padding='max_length', max_length=max_seq_len, truncation=True, return_tensors=\"pt\")\n","    labels = torch.tensor(df.valid.values, dtype=torch.float)\n","    return title_tokens, labels"],"metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["news_model = NewsModel(bert_model=bert_model_path).to(device)\n","optimiser = AdamW(news_model.parameters(), lr=1e-5)\n","train_set, test_set = train_test_split(news_source, test_size=0.2, random_state=7)\n","\n","text_tokens, labels = covertTokenFormat(train_set, bert_model_path, max_seq_length)\n","train_data = TensorDataset(text_tokens.input_ids, text_tokens.attention_mask, labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)"],"metadata":{"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4870702dba9d42faaa58b2df51eaba46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443313d3e88c43968026eee0145cba3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"712c707a0ed94dec85606fe0fcaa4720"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d458bade0d464ae0803960280167b4d6"}},"metadata":{}}]},{"cell_type":"code","source":["def news_bert_report(pred, label):\n","    tar_y = label.squeeze()\n","    pred_y = []\n","    for item in pred.squeeze():\n","        if item >= 0.5:\n","            pred_y.append(1)\n","        else:\n","            pred_y.append(0)\n","    f1 = f1_score(tar_y, pred_y)\n","    print(\"f1-score: \", f1)\n","    acc = accuracy_score(tar_y, pred_y)\n","    print(\"accuracy: \", acc)"],"metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"source":["### 进行训练\n","- 实际上根据在training set和testing set上的表现制定一套早停策略更好\n","- BERT非常强大，快速的在训练集上逼近了99.95+%"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["from torch.nn import functional as F\n","news_model = news_model.to(device)\n","news_model.train()\n","for epoch in range(epoch_num): \n","    epoch_loss = 0\n","    pred_lis = torch.Tensor()\n","    label_lis = torch.Tensor()\n","    for batch, (token_ids, attn_mask, label) in enumerate(train_dataloader):\n","        # keep all the parameters in the same device\n","        token_ids = token_ids.to(device)\n","        attn_mask = attn_mask.to(device)\n","        label = label.to(device)\n","        # the output will be in the same device with the model\n","        outputs = news_model(token_ids, attn_mask)\n","        loss = F.binary_cross_entropy(outputs.squeeze(), label)\n","        # do the backprop and update the parameters\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        epoch_loss += loss.cpu().data.numpy()\n","        pred_lis = torch.cat([pred_lis, outputs.cpu().squeeze()])\n","        label_lis = torch.cat([label_lis, label.cpu().squeeze()])\n","        if batch % 50 == 0:\n","            print(\"Current batch loss :\", loss.cpu().data.numpy())\n","    print(\"Now epoch :\", epoch+1, \" Total epoch loss is: \", epoch_loss)\n","    news_bert_report(pred_lis.detach().numpy(), label_lis.detach().numpy())"],"metadata":{"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Current batch loss : 0.69488025\nCurrent batch loss : 0.3863423\nCurrent batch loss : 0.19158298\nCurrent batch loss : 0.09455851\nCurrent batch loss : 0.031617053\nCurrent batch loss : 0.109588556\nCurrent batch loss : 0.02393411\nCurrent batch loss : 0.084877014\nCurrent batch loss : 0.022406645\nCurrent batch loss : 0.01850613\nNow epoch : 1  Total epoch loss is:  61.19065617257729\nf1-score:  0.9732358680140957\naccuracy:  0.9702867858643991\nCurrent batch loss : 0.005929833\nCurrent batch loss : 0.008962766\nCurrent batch loss : 0.04420775\nCurrent batch loss : 0.08559545\nCurrent batch loss : 0.053183675\nCurrent batch loss : 0.0019030205\nCurrent batch loss : 0.001381191\nCurrent batch loss : 0.10759225\nCurrent batch loss : 0.0022155712\nCurrent batch loss : 0.0036287848\nNow epoch : 2  Total epoch loss is:  5.632405258889776\nf1-score:  0.9969665734059786\naccuracy:  0.9966697921044974\nCurrent batch loss : 0.00081905833\nCurrent batch loss : 0.08888218\nCurrent batch loss : 0.0011167352\nCurrent batch loss : 0.0067362455\nCurrent batch loss : 0.014054378\nCurrent batch loss : 0.0010038681\nCurrent batch loss : 0.0005517043\nCurrent batch loss : 0.00044387838\nCurrent batch loss : 0.004648841\nCurrent batch loss : 0.002543316\nNow epoch : 3  Total epoch loss is:  3.290774488792522\nf1-score:  0.9983215053446804\naccuracy:  0.998157069417052\nCurrent batch loss : 0.1128003\nCurrent batch loss : 0.00058553193\nCurrent batch loss : 0.00044043502\nCurrent batch loss : 0.00096610765\nCurrent batch loss : 0.0003082651\nCurrent batch loss : 0.00028307142\nCurrent batch loss : 0.00025527828\nCurrent batch loss : 0.00027438768\nCurrent batch loss : 0.000869742\nCurrent batch loss : 0.00037016947\nNow epoch : 4  Total epoch loss is:  1.146693637201679\nf1-score:  0.9997054838899688\naccuracy:  0.9996766788450968\n","output_type":"stream"}]},{"cell_type":"code","source":["torch.save(news_model, './news_model_24_03_01.pkl')"],"metadata":{"trusted":true},"execution_count":31,"outputs":[]},{"source":["### Kaggle的GPU也塞不下了，不过至少测试机集中这一百条数据全部正确。"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["news_model.eval()\n","test_tokens, test_labels = covertTokenFormat(test_set[:100], bert_model_path, max_seq_length)\n","test_ids = test_tokens.input_ids.to(device)\n","test_masks = test_tokens.attention_mask.to(device)\n","test_pred = news_model(test_ids, test_masks)\n","news_bert_report(test_pred.cpu().squeeze().detach().numpy(), test_labels)\n","\n","torch.cuda.empty_cache()"],"metadata":{"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"f1-score:  1.0\naccuracy:  1.0\n","output_type":"stream"}]},{"source":["## 利用GLOVE预训练词向量和GRU进行真假新闻判定\n","- 将会忽略正文中所有stopwords和标点符号\n","- 利用了golve 6b 50d的预训练词向量"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["import torch\n","import string\n","import torch.nn.utils.rnn as rnn_utils\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from torchtext.vocab import GloVe\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n","\n","import pandas as pd\n","news_source = pd.read_csv('./NewsAna.csv')\n","news_source.reset_index()\n","news_source = news_source.sample(frac=1.)\n","\n","stop_words = set(stopwords.words('english'))\n","punctuations = set(string.punctuation)\n","cache_dir = './glove'\n","glove = GloVe(name='6B', dim=50, cache=cache_dir)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","epoch_num = 30\n","batch_size = 128\n","lr = .001\n","input_dim = 50\n","output_dim = 1\n","gru_num_layers = 2"],"metadata":{"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"./glove/glove.6B.zip: 862MB [03:30, 4.10MB/s]                               \n100%|█████████▉| 399999/400000 [00:14<00:00, 27930.42it/s]\n","output_type":"stream"}]},{"source":["### 定义GRU网络，使用预训练词向量的话，可以略过Embedding层\n","- 由于处理的是变长的数据，因此利用RNN的pad和unpad操作，捕捉每个输入句子的真正结束位置\n","- 指定了`batch_first=True`，故数据x进入rnn前不必交换batch（第0维）和sentence_length（第一维）位置"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["class NewsGRUModel(torch.nn.Module):\n","    def __init__(self, input_dim, output_dim, vocab_size=0, gru_num_layers=1, bidirectional=False, dropout=.3, hidden_layers = [128, 64, 128]):\n","        super(NewsGRUModel, self).__init__()\n","        self.input_dim = input_dim\n","        self.hidden_dim = input_dim // 2 if bidirectional else input_dim\n","        self.output_dim = output_dim\n","        self.gru_num_layers = gru_num_layers\n","        # Embedding\n","        if not vocab_size == 0:\n","            self.embed = torch.nn.Embedding(vocab_size, input_dim)\n","        # GRUs\n","        self.gru_layer = torch.nn.GRU(\n","            input_size=self.input_dim, \n","            hidden_size=self.hidden_dim, \n","            num_layers=self.gru_num_layers, \n","            bidirectional=bidirectional, \n","            batch_first=True,\n","            dropout=dropout\n","        )\n","        # The FFN to adjust the outputs\n","        if hidden_layers and not len(hidden_layers) == 0:\n","            # the dim is not changed through the two GRU layer\n","            hidden_list = [torch.nn.Linear(self.input_dim, hidden_layers[0])]\n","            for idx in range(len(hidden_layers) - 1):\n","                hidden_list.append(torch.nn.Linear(hidden_layers[idx], hidden_layers[idx + 1]))\n","            self.hidden_layer_list = torch.nn.ModuleList(hidden_list)\n","            # init the weights\n","            for layer in self.hidden_layer_list: \n","                torch.nn.init.kaiming_normal_(layer.weight.data)\n","            self.hidden_out_dim = hidden_layers[-1]\n","        else:\n","            self.hidden_layer_list = []\n","            self.hidden_out_dim = self.input_dim\n","        # Output layer\n","        self.output = torch.nn.Linear(self.hidden_out_dim, self.output_dim)\n","        torch.nn.init.kaiming_normal_(self.output.weight.data)\n","        # Other functions\n","        self.activate = torch.nn.ReLU()\n","        self.dropout = torch.nn.Dropout(dropout)\n","    \n","    def forward(self, x, x_len, pretrained_embed=False):\n","        if not pretrained_embed:\n","            x = self.embed(x)\n","        # pack padded seq\n","        x = rnn_utils.pack_padded_sequence(x, x_len, batch_first=True, enforce_sorted=False)\n","        # GRU layer\n","        output, hidden_info = self.gru_layer(x)\n","        # pad packed seq\n","        output, length = rnn_utils.pad_packed_sequence(output, batch_first=True)\n","        # gather the output from the last unpad token\n","        fin_cell_outputs = []\n","        for idx in range(len(length)):\n","            fin_cell_outputs.append(output[idx][length[idx]-1])\n","        # stack the output\n","        output = torch.stack(fin_cell_outputs)\n","        # ffn process\n","        for layer in self.hidden_layer_list:\n","            output = layer(output)\n","            output = self.activate(output)\n","            output = self.dropout(output)\n","        # output layer, get logits\n","        output = self.output(output)\n","        return output"],"metadata":{"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# 将句子转化为tokens，用了Glove的预训练词向量\n","def covertTextToGolveVec(df):\n","    golve_vecs = []\n","    titles = df.text.values\n","    for title in titles:\n","        tokens = word_tokenize(title.lower())\n","        for token in tokens:\n","            if token in stop_words or token in punctuations:\n","                tokens.remove(token)\n","        golve_vecs.append(glove.get_vecs_by_tokens(tokens))\n","    return golve_vecs"],"metadata":{"trusted":true},"execution_count":35,"outputs":[]},{"source":["### 定义Dataset和collate_fn方法：\n","- Dataset用于数据存储，我们希望从Dataset中每次取出的批量数据是Tensor格式的，即可以直接放入GPU中进行批处理\n","- collate_fn方法定义了每次从Dataset中取出一批数据的方法，也就是说，在这里对变长的句子进行pad，并保存每个句子的实际长度\n","- 故如果能将Dataset中的数据按长度升序或降序排列可以进一步减少计算开销（此处未进行）"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["class NewsDataset(Dataset):\n","    def __init__(self, train_x, train_y):\n","        self.train_x = train_x\n","        self.train_y = train_y\n","    def __len__(self):\n","        return len(self.train_y)\n","    def __getitem__(self, idx):\n","        idx -= 1\n","        return self.train_x[idx], self.train_y[idx]\n","    \n","def collate_fn(train_data):\n","    (train_data, train_label) = zip(*train_data)\n","    data_length = [len(data) for data in train_data]\n","    train_data = rnn_utils.pad_sequence(train_data, batch_first=True, padding_value=0)\n","    train_label = torch.Tensor(train_label)\n","    return train_data, train_label, data_length"],"metadata":{"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["train_set, test_set = train_test_split(news_source, test_size=0.2, random_state=7)\n","news_vecs = covertTextToGolveVec(train_set)\n","label_vecs = train_set.valid.values\n","\n","train_dataset = NewsDataset(news_vecs, label_vecs)\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn)\n","\n","news_gru = NewsGRUModel(input_dim=input_dim, output_dim=output_dim, gru_num_layers=gru_num_layers).to(device)\n","optimiser = torch.optim.Adam(news_gru.parameters(), lr=lr)\n","loss_func = torch.nn.MSELoss()"],"metadata":{"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def news_gru_report(pred, label):\n","    tar_y = label.squeeze()\n","    pred_y = []\n","    for item in pred.squeeze():\n","        if item >= 0.5:\n","            pred_y.append(1)\n","        else:\n","            pred_y.append(0)\n","    f1 = f1_score(tar_y, pred_y)\n","    print(\"f1-score: \", f1)\n","    acc = accuracy_score(tar_y, pred_y)\n","    print(\"accuracy: \", acc)\n","    cm = confusion_matrix(tar_y, pred_y)\n","    print(\"confusion matrix:\\n\",cm)"],"metadata":{"trusted":true},"execution_count":38,"outputs":[]},{"source":["### 看来这份数据集中的真假新闻差异确实不小，GRU模型各项指标也是迅速潘升到99.8+%\n","- 两个模型的训练过程是很草率的，想要一个细致可接受的结果的话还要加入更多训练策略\n","- 不过用BERT的话，要跑K-Fold实在是太吃力了（指资源）"],"cell_type":"markdown","metadata":{}},{"cell_type":"code","source":["news_gru = news_gru.to(device)\n","news_gru.train()\n","for epoch in range(epoch_num):\n","    epoch_loss = 0\n","    pred_lis = torch.Tensor()\n","    label_lis = torch.Tensor()\n","    for batch_idx, (data, label, length) in enumerate(train_dataloader):\n","        input_vec = data.to(device)\n","        label = label.to(device)\n","        pred = news_gru(input_vec, length, True)\n","        loss = loss_func(pred.squeeze(), label.squeeze())\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","        epoch_loss += loss.cpu().data.numpy()\n","        pred_lis = torch.cat([pred_lis, pred.cpu().squeeze()])\n","        label_lis = torch.cat([label_lis, label.cpu().squeeze()])\n","    if (epoch + 1) % 10 == 0 or epoch == 0:\n","        news_gru_report(pred_lis.detach().numpy(), label_lis.detach().numpy())\n","        print(\"Current epoch: \", epoch + 1, \" Total loss: \", epoch_loss)"],"metadata":{"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"f1-score:  0.8509674887553959\naccuracy:  0.8403763458243073\nconfusion matrix:\n [[11897  2065]\n [ 2872 14095]]\nCurrent epoch:  1  Total loss:  32.24637720733881\nf1-score:  0.9941675503711558\naccuracy:  0.9935982411329173\nconfusion matrix:\n [[13856   106]\n [   92 16875]]\nCurrent epoch:  10  Total loss:  2.1509476064238697\nf1-score:  0.9964060566782537\naccuracy:  0.9960554819101813\nconfusion matrix:\n [[13895    67]\n [   55 16912]]\nCurrent epoch:  20  Total loss:  1.5841168414335698\nf1-score:  0.9982031871336418\naccuracy:  0.9980277409550907\nconfusion matrix:\n [[13924    38]\n [   23 16944]]\nCurrent epoch:  30  Total loss:  1.1582776526920497\n","output_type":"stream"}]},{"cell_type":"code","source":["news_gru.eval()\n","news_gru.to('cpu')\n","test_news = covertTextToGolveVec(test_set[:50])\n","test_labels = test_set.valid.values[:50]\n","test_length = [len(data) for data in test_news]\n","test_data = rnn_utils.pad_sequence(test_news, batch_first=True, padding_value=0)\n","test_pred = news_gru(test_data, test_length, True)\n","news_gru_report(test_pred, test_labels)\n","\n","torch.cuda.empty_cache()"],"metadata":{"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"f1-score:  1.0\naccuracy:  1.0\nconfusion matrix:\n [[25  0]\n [ 0 25]]\n","output_type":"stream"}]},{"cell_type":"code","source":[],"metadata":{},"execution_count":null,"outputs":[]}]}